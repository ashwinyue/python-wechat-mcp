#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬å–ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å¾®ä¿¡çˆ¬è™«åŠŸèƒ½
"""

import json
import sys
from weixin_spider import WeixinSpider

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬å–ç¤ºä¾‹")
    print("=" * 60)
    
    # ç¤ºä¾‹URLï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…çš„å¾®ä¿¡æ–‡ç« URLï¼‰
    example_url = "https://mp.weixin.qq.com/s/example-article-url"
    
    print(f"\nğŸ“ ç¤ºä¾‹è¯´æ˜ï¼š")
    print(f"æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨å¾®ä¿¡çˆ¬è™«åŠŸèƒ½çˆ¬å–å…¬ä¼—å·æ–‡ç« ")
    print(f"è¯·å°†ç¤ºä¾‹URLæ›¿æ¢ä¸ºå®é™…çš„å¾®ä¿¡æ–‡ç« URL")
    print(f"ç¤ºä¾‹URL: {example_url}")
    
    # è·å–ç”¨æˆ·è¾“å…¥
    print(f"\nğŸ”— è¯·è¾“å…¥è¦çˆ¬å–çš„å¾®ä¿¡æ–‡ç« URLï¼š")
    print(f"ï¼ˆç›´æ¥æŒ‰å›è½¦ä½¿ç”¨ç¤ºä¾‹URLï¼Œè¾“å…¥'quit'é€€å‡ºï¼‰")
    
    user_input = input().strip()
    
    if user_input.lower() == 'quit':
        print("ğŸ‘‹ é€€å‡ºç¨‹åº")
        return
    
    if user_input:
        url = user_input
    else:
        url = example_url
        print(f"ä½¿ç”¨ç¤ºä¾‹URL: {url}")
    
    # éªŒè¯URLæ ¼å¼
    if not url.startswith("https://mp.weixin.qq.com/"):
        print("âŒ é”™è¯¯ï¼šURLå¿…é¡»ä»¥ https://mp.weixin.qq.com/ å¼€å¤´")
        return
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    print(f"\nğŸ•·ï¸ åˆå§‹åŒ–çˆ¬è™«...")
    try:
        spider = WeixinSpider(
            headless=True,        # ä½¿ç”¨æ— å¤´æ¨¡å¼
            wait_time=10,         # ç­‰å¾…æ—¶é—´10ç§’
            download_images=True  # ä¸‹è½½å›¾ç‰‡
        )
        print("âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥Chromeæµè§ˆå™¨æ˜¯å¦å·²å®‰è£…")
        return
    
    try:
        # çˆ¬å–æ–‡ç« 
        print(f"\nğŸ“„ å¼€å§‹çˆ¬å–æ–‡ç« ...")
        print(f"URL: {url}")
        
        article_data = spider.crawl_article_by_url(url)
        
        if article_data:
            print(f"âœ… æ–‡ç« çˆ¬å–æˆåŠŸï¼")
            
            # æ˜¾ç¤ºæ–‡ç« ä¿¡æ¯
            print(f"\nğŸ“Š æ–‡ç« ä¿¡æ¯ï¼š")
            print(f"æ ‡é¢˜: {article_data.get('title', 'æœªçŸ¥')}")
            print(f"ä½œè€…: {article_data.get('author', 'æœªçŸ¥')}")
            print(f"å‘å¸ƒæ—¶é—´: {article_data.get('publish_time', 'æœªçŸ¥')}")
            print(f"å­—æ•°: {article_data.get('word_count', 0)}")
            print(f"å›¾ç‰‡æ•°é‡: {len(article_data.get('images', []))}")
            
            # ä¿å­˜æ–‡ç« 
            print(f"\nğŸ’¾ ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶...")
            success = spider.save_article_to_file(article_data)
            
            if success:
                print(f"âœ… æ–‡ç« ä¿å­˜æˆåŠŸï¼")
                print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨ articles/ ç›®å½•ä¸‹")
                print(f"åŒ…å«æ ¼å¼: JSONã€TXTã€HTML")
                if article_data.get('images'):
                    print(f"ğŸ“¸ å›¾ç‰‡ä¿å­˜åœ¨å¯¹åº”çš„ images/ å­ç›®å½•ä¸­")
            else:
                print(f"âŒ æ–‡ç« ä¿å­˜å¤±è´¥")
            
            # æ˜¾ç¤ºæ–‡ç« å†…å®¹é¢„è§ˆ
            content = article_data.get('content', '')
            if content:
                print(f"\nğŸ“– æ–‡ç« å†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰ï¼š")
                print("-" * 40)
                print(content[:200] + "..." if len(content) > 200 else content)
                print("-" * 40)
            
        else:
            print(f"âŒ æ–‡ç« çˆ¬å–å¤±è´¥")
            print(f"ğŸ’¡ è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        
    finally:
        # å…³é—­çˆ¬è™«
        print(f"\nğŸ”„ å…³é—­çˆ¬è™«...")
        spider.close()
        print(f"âœ… çˆ¬è™«å·²å…³é—­")
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ‰ ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print(f"ğŸ’¡ å¦‚éœ€åœ¨MCPå®¢æˆ·ç«¯ä¸­ä½¿ç”¨ï¼Œè¯·è¿è¡Œ: make run-all")
    print(f"=" * 60)

if __name__ == "__main__":
    main() 