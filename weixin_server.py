#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬å– MCP æœåŠ¡å™¨
æä¾›å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬å–å’Œæ–‡ä»¶ä¿å­˜åŠŸèƒ½
"""

import json
import logging
import os
import sys
from typing import Any, Dict, List, Optional
from datetime import datetime
import re
from collections import Counter

# MCP imports
from mcp.server.fastmcp import FastMCP

# å¯¼å…¥å¾®ä¿¡çˆ¬è™«
from weixin_spider import WeixinSpider

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastMCPæœåŠ¡å™¨å®ä¾‹
mcp = FastMCP("weixin-spider")

# å…¨å±€çˆ¬è™«å®ä¾‹
spider_instance: Optional[WeixinSpider] = None

def get_spider_instance() -> WeixinSpider:
    """è·å–çˆ¬è™«å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global spider_instance
    if spider_instance is None:
        try:
            spider_instance = WeixinSpider(
                headless=True,  # MCPæœåŠ¡å™¨ä¸­ä½¿ç”¨æ— å¤´æ¨¡å¼
                wait_time=10,
                download_images=True
            )
            logger.info("çˆ¬è™«å®ä¾‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"çˆ¬è™«å®ä¾‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–çˆ¬è™«å®ä¾‹: {e}")
    
    # æ£€æŸ¥é©±åŠ¨æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
    if spider_instance.driver is None:
        logger.warning("æ£€æµ‹åˆ°é©±åŠ¨å·²å¤±æ•ˆï¼Œé‡æ–°åˆå§‹åŒ–...")
        try:
            spider_instance.setup_driver(headless=True)
            logger.info("é©±åŠ¨é‡æ–°åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"é©±åŠ¨é‡æ–°åˆå§‹åŒ–å¤±è´¥: {e}")
            # åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹
            try:
                spider_instance = WeixinSpider(
                    headless=True,
                    wait_time=10,
                    download_images=True
                )
                logger.info("åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹æˆåŠŸ")
            except Exception as new_e:
                logger.error(f"åˆ›å»ºæ–°çˆ¬è™«å®ä¾‹å¤±è´¥: {new_e}")
                raise RuntimeError(f"æ— æ³•åˆ›å»ºçˆ¬è™«å®ä¾‹: {new_e}")
    
    return spider_instance

@mcp.tool()
async def crawl_weixin_article(url: str, download_images: bool = True, custom_filename: str = None) -> str:
    """
    çˆ¬å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹å¹¶ä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        url: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çš„URLé“¾æ¥ï¼Œå¿…é¡»ä»¥ https://mp.weixin.qq.com/ å¼€å¤´
        download_images: æ˜¯å¦ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡ï¼Œé»˜è®¤ä¸º true
        custom_filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸æä¾›å°†ä½¿ç”¨æ–‡ç« æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
    
    Returns:
        çˆ¬å–ç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        # éªŒè¯URL
        if not url or not isinstance(url, str) or not url.startswith("https://mp.weixin.qq.com/"):
            return json.dumps({
                "status": "error",
                "message": "æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URLï¼Œå¿…é¡»ä»¥ https://mp.weixin.qq.com/ å¼€å¤´"
            }, ensure_ascii=False, indent=2)
        
        logger.info(f"å¼€å§‹çˆ¬å–æ–‡ç« : {url}")
        
        # è·å–çˆ¬è™«å®ä¾‹
        spider = get_spider_instance()
        
        # è®¾ç½®æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        spider.download_images = download_images
        
        # çˆ¬å–æ–‡ç« 
        article_data = spider.crawl_article_by_url(url)
        
        if not article_data:
            return json.dumps({
                "status": "error",
                "message": "æ— æ³•è·å–æ–‡ç« å†…å®¹ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®æˆ–ç½‘ç»œè¿æ¥"
            }, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶
        success = spider.save_article_to_file(article_data, custom_filename)
        
        if success:
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "status": "success",
                "message": "æ–‡ç« çˆ¬å–æˆåŠŸ",
                "article": {
                    "title": article_data.get("title", ""),
                    "author": article_data.get("author", ""),
                    "publish_time": article_data.get("publish_time", ""),
                    "url": article_data.get("url", ""),
                    "content_length": len(article_data.get("content", "")),
                    "word_count": article_data.get("word_count", 0),
                    "images_count": len(article_data.get("images", [])),
                    "crawl_time": article_data.get("crawl_time", "")
                },
                "files_saved": {
                    "json": True,
                    "txt": True,
                    "html": True,
                    "images": download_images
                }
            }
            
            if download_images:
                images = article_data.get("images", [])
                success_count = sum(1 for img in images if img.get("download_success", False))
                result["article"]["images_downloaded"] = f"{success_count}/{len(images)}"
            
            return json.dumps(result, ensure_ascii=False, indent=2)
        else:
            return json.dumps({
                "status": "error",
                "message": "æ–‡ç« çˆ¬å–æˆåŠŸä½†ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™"
            }, ensure_ascii=False, indent=2)
            
    except Exception as e:
        logger.error(f"çˆ¬å–æ–‡ç« å¤±è´¥: {e}")
        return json.dumps({
            "status": "error",
            "message": f"çˆ¬å–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def analyze_article_content(article_data: dict, analysis_type: str = "full") -> str:
    """
    åˆ†æå·²çˆ¬å–çš„æ–‡ç« å†…å®¹ï¼Œæä¾›æ‘˜è¦å’Œç»Ÿè®¡ä¿¡æ¯
    
    Args:
        article_data: æ–‡ç« æ•°æ®å¯¹è±¡ï¼ŒåŒ…å«æ ‡é¢˜ã€å†…å®¹ç­‰ä¿¡æ¯
        analysis_type: åˆ†æç±»å‹ï¼šsummary(æ‘˜è¦), keywords(å…³é”®è¯), images(å›¾ç‰‡ä¿¡æ¯), full(å®Œæ•´åˆ†æ)
    
    Returns:
        åˆ†æç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not article_data or not isinstance(article_data, dict):
            return json.dumps({
                "status": "error",
                "message": "article_data å¿…é¡»æ˜¯å­—å…¸æ ¼å¼çš„æ–‡ç« æ•°æ®"
            }, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ†ææ–‡ç« å†…å®¹: analysis_type={analysis_type}")
        
        result = {"analysis_type": analysis_type}
        
        if analysis_type in ["summary", "full"]:
            content = article_data.get("content", "")
            result["summary"] = {
                "title": article_data.get("title", ""),
                "author": article_data.get("author", ""),
                "publish_time": article_data.get("publish_time", ""),
                "word_count": len(content),
                "paragraph_count": len([p for p in content.split('\n') if p.strip()]),
                "estimated_reading_time": f"{max(1, len(content) // 300)} åˆ†é’Ÿ"
            }
        
        if analysis_type in ["keywords", "full"]:
            content = article_data.get("content", "")
            # æ”¹è¿›çš„å…³é”®è¯æå–ï¼ˆåŸºäºè¯é¢‘ï¼‰
            # æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦å’ŒåŸºæœ¬æ ‡ç‚¹
            cleaned_text = re.sub(r'[^\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', ' ', content)
            
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆåŸºäºæ ‡ç‚¹å’Œç©ºæ ¼ï¼‰
            # ç§»é™¤å¸¸è§åœç”¨è¯
            stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'å®ƒ', 'ä»–', 'å¥¹', 'ä»¬', 'æ¥', 'è¿‡', 'æ—¶', 'å¤§', 'å°', 'å¤š', 'å°‘', 'å¯ä»¥', 'èƒ½å¤Ÿ', 'åº”è¯¥', 'å¿…é¡»', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'ç°åœ¨', 'å·²ç»', 'è¿˜æ˜¯', 'åªæ˜¯', 'æˆ–è€…', 'ä»¥åŠ', 'å¹¶ä¸”', 'è€Œä¸”', 'ä¸è¿‡', 'è™½ç„¶', 'å°½ç®¡', 'é™¤äº†', 'é€šè¿‡', 'å…³äº', 'å¯¹äº', 'ç”±äº', 'ä¸ºäº†', 'æ ¹æ®', 'æŒ‰ç…§', 'ä¾æ®', 'åŸºäº'}
            
            # æå–2-4å­—çš„ä¸­æ–‡è¯ç»„
            words = []
            text_parts = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š\s]+', cleaned_text)
            
            for part in text_parts:
                if len(part) >= 2:
                    # æå–2-4å­—çš„è¿ç»­ä¸­æ–‡å­—ç¬¦ï¼Œä¼˜å…ˆæå–å®Œæ•´è¯æ±‡
                    for i in range(len(part)):
                        for length in [4, 3, 2]:  # ä¼˜å…ˆæå–é•¿è¯
                            if i + length <= len(part):
                                word = part[i:i+length]
                                if (len(word) == length and 
                                    word not in stop_words and 
                                    re.match(r'^[\u4e00-\u9fff]+$', word) and
                                    not any(word.endswith(suffix) for suffix in ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä¼š', 'ç€', 'æ²¡', 'çœ‹', 'å¥½', 'è¿™', 'é‚£', 'å®ƒ', 'ä»–', 'å¥¹', 'ä»¬', 'æ¥', 'è¿‡', 'æ—¶', 'å¤§', 'å°', 'å¤š', 'å°‘'])):
                                    words.append(word)
            
            # ç»Ÿè®¡è¯é¢‘
            word_freq = Counter(words)
            top_keywords = word_freq.most_common(15)
            
            # è¿‡æ»¤æ‰é¢‘æ¬¡å¤ªä½çš„è¯ï¼ˆå°‘äº2æ¬¡ï¼‰
            filtered_keywords = [(word, count) for word, count in top_keywords if count >= 2]
            
            result["keywords"] = [{"word": word, "count": count} for word, count in filtered_keywords[:10]]
        
        if analysis_type in ["images", "full"]:
            images = article_data.get("images", [])
            result["images_analysis"] = {
                "total_images": len(images),
                "downloaded_images": sum(1 for img in images if img.get("download_success", False)),
                "failed_images": sum(1 for img in images if not img.get("download_success", False)),
                "image_types": list(set([
                    img.get("url", "").split(".")[-1].lower() 
                    for img in images 
                    if img.get("url") and "." in img.get("url")
                ]))
            }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"åˆ†ææ–‡ç« å†…å®¹å¤±è´¥: {e}")
        return json.dumps({
            "status": "error",
            "message": f"åˆ†æå¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def get_article_statistics(article_data: dict) -> str:
    """
    è·å–æ–‡ç« çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        article_data: æ–‡ç« æ•°æ®å¯¹è±¡
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not article_data or not isinstance(article_data, dict):
            return json.dumps({
                "status": "error",
                "message": "article_data å¿…é¡»æ˜¯å­—å…¸æ ¼å¼çš„æ–‡ç« æ•°æ®"
            }, ensure_ascii=False, indent=2)
        
        content = article_data.get("content", "")
        images = article_data.get("images", [])
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "basic_info": {
                "title": article_data.get("title", ""),
                "author": article_data.get("author", ""),
                "publish_time": article_data.get("publish_time", ""),
                "crawl_time": article_data.get("crawl_time", ""),
                "url": article_data.get("url", "")
            },
            "content_stats": {
                "total_characters": len(content),
                "total_words": article_data.get("word_count", len(content)),
                "paragraphs": len([p for p in content.split('\n') if p.strip()]),
                "estimated_reading_time": f"{max(1, len(content) // 300)} åˆ†é’Ÿ"
            },
            "image_stats": {
                "total_images": len(images),
                "downloaded_images": sum(1 for img in images if img.get("download_success", False)),
                "failed_downloads": sum(1 for img in images if not img.get("download_success", False))
            }
        }
        
        return json.dumps(stats, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ç« ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return json.dumps({
            "status": "error",
            "message": f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def generate_reading_notes(article_data: dict, note_style: str = "summary") -> str:
    """
    æ ¹æ®æ–‡ç« å†…å®¹ç”Ÿæˆè¯»ä¹¦ç¬”è®°
    
    Args:
        article_data: æ–‡ç« æ•°æ®å¯¹è±¡ï¼ŒåŒ…å«æ ‡é¢˜ã€å†…å®¹ç­‰ä¿¡æ¯
        note_style: ç¬”è®°é£æ ¼ï¼šsummary(æ‘˜è¦å¼), detailed(è¯¦ç»†å¼), mind_map(æ€ç»´å¯¼å›¾å¼), key_points(è¦ç‚¹å¼), one_sentence(ä¸€å¥è¯æ€»ç»“)
    
    Returns:
        ç”Ÿæˆçš„è¯»ä¹¦ç¬”è®°å†…å®¹
    """
    try:
        if not article_data or not isinstance(article_data, dict):
            return json.dumps({
                "status": "error",
                "message": "article_data å¿…é¡»æ˜¯å­—å…¸æ ¼å¼çš„æ–‡ç« æ•°æ®"
            }, ensure_ascii=False, indent=2)
        
        logger.info(f"ç”Ÿæˆè¯»ä¹¦ç¬”è®°: note_style={note_style}")
        
        # è·å–æ–‡ç« åŸºæœ¬ä¿¡æ¯
        title = article_data.get("title", "æœªçŸ¥æ ‡é¢˜")
        author = article_data.get("author", "æœªçŸ¥ä½œè€…")
        publish_time = article_data.get("publish_time", "æœªçŸ¥æ—¶é—´")
        content = article_data.get("content", "")
        word_count = article_data.get("word_count", len(content))
        
        # åˆ†æå…³é”®è¯
        keywords_result = await analyze_article_content(article_data, "keywords")
        keywords_data = json.loads(keywords_result)
        keywords = keywords_data.get("keywords", [])
        
        # æ ¹æ®ä¸åŒé£æ ¼ç”Ÿæˆç¬”è®°
        if note_style == "summary":
            notes = _generate_summary_notes(title, author, publish_time, content, word_count, keywords)
        elif note_style == "detailed":
            notes = _generate_detailed_notes(title, author, publish_time, content, word_count, keywords)
        elif note_style == "mind_map":
            notes = _generate_mind_map_notes(title, author, publish_time, content, word_count, keywords)
        elif note_style == "key_points":
            notes = _generate_key_points_notes(title, author, publish_time, content, word_count, keywords)
        elif note_style == "one_sentence":
            # ç”Ÿæˆä¸€å¥è¯æ€»ç»“
            one_sentence = _generate_one_sentence_from_content(title, author, [kw.get("word", "") for kw in keywords[:3]], [])
            notes = f"# ä¸€å¥è¯è¯»ä¹¦ç¬”è®°ï¼š{title}\n\n**{one_sentence}**\n\n---\n*ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}*"
        else:
            notes = _generate_summary_notes(title, author, publish_time, content, word_count, keywords)
        
        return json.dumps({
            "status": "success",
            "note_style": note_style,
            "notes": notes,
            "word_count": len(notes),
            "generated_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆè¯»ä¹¦ç¬”è®°å¤±è´¥: {e}")
        return json.dumps({
            "status": "error",
            "message": f"ç”Ÿæˆç¬”è®°å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)

def _generate_summary_notes(title, author, publish_time, content, word_count, keywords):
    """ç”Ÿæˆæ‘˜è¦å¼è¯»ä¹¦ç¬”è®°"""
    # æå–æ–‡ç« ä¸»è¦æ®µè½
    paragraphs = [p.strip() for p in content.split('\n') if p.strip() and len(p.strip()) > 20]
    
    # é€‰æ‹©å…³é”®æ®µè½ï¼ˆå¼€å¤´ã€ä¸­é—´ã€ç»“å°¾ï¼‰
    key_paragraphs = []
    if len(paragraphs) >= 3:
        key_paragraphs.append(paragraphs[0])  # å¼€å¤´
        key_paragraphs.append(paragraphs[len(paragraphs)//2])  # ä¸­é—´
        key_paragraphs.append(paragraphs[-1])  # ç»“å°¾
    else:
        key_paragraphs = paragraphs
    
    # ç”Ÿæˆå…³é”®è¯åˆ—è¡¨
    keyword_list = "ã€".join([kw.get("word", "") for kw in keywords[:8]])
    
    notes = f"""# è¯»ä¹¦ç¬”è®°ï¼š{title}

## ğŸ“š æ–‡ç« ä¿¡æ¯
- **æ ‡é¢˜**ï¼š{title}
- **ä½œè€…**ï¼š{author}
- **å‘å¸ƒæ—¶é—´**ï¼š{publish_time}
- **å­—æ•°**ï¼š{word_count}å­—
- **ç¬”è®°ç”Ÿæˆæ—¶é—´**ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}

## ğŸ¯ æ ¸å¿ƒè¦ç‚¹

### å…³é”®è¯
{keyword_list}

### ä¸»è¦å†…å®¹
{chr(10).join([f"{i+1}. {para[:200]}..." if len(para) > 200 else f"{i+1}. {para}" for i, para in enumerate(key_paragraphs)])}

## ğŸ’¡ ä¸ªäººæ€è€ƒ
- è¿™ç¯‡æ–‡ç« ä¸»è¦è®²è¿°äº†{title.replace('èŠä¸€èŠæˆ‘æ˜¯å¦‚ä½•å­¦ä¹ ', '').replace('çš„', '')}ç›¸å…³å†…å®¹
- ä½œè€…{author}åˆ†äº«äº†è‡ªå·±çš„ç»éªŒå’Œè§è§£
- å€¼å¾—è¿›ä¸€æ­¥æ€è€ƒå’Œå®è·µçš„åœ°æ–¹ï¼š
  â–¡ 
  â–¡ 
  â–¡ 

## ğŸ“ è¡ŒåŠ¨è®¡åˆ’
- [ ] 
- [ ] 
- [ ] 

---
*æœ¬ç¬”è®°ç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œå»ºè®®ç»“åˆä¸ªäººç†è§£è¿›è¡Œè¡¥å……å’Œå®Œå–„*
"""
    return notes

def _generate_detailed_notes(title, author, publish_time, content, word_count, keywords):
    """ç”Ÿæˆè¯¦ç»†å¼è¯»ä¹¦ç¬”è®°"""
    # æŒ‰æ®µè½åˆ†æå†…å®¹
    paragraphs = [p.strip() for p in content.split('\n') if p.strip() and len(p.strip()) > 10]
    
    # ç”Ÿæˆå…³é”®è¯åˆ†æ
    keyword_analysis = "\n".join([f"- **{kw.get('word', '')}**ï¼šå‡ºç°{kw.get('count', 0)}æ¬¡" for kw in keywords[:10]])
    
    notes = f"""# è¯¦ç»†è¯»ä¹¦ç¬”è®°ï¼š{title}

## ğŸ“– åŸºæœ¬ä¿¡æ¯
| é¡¹ç›® | å†…å®¹ |
|------|------|
| æ ‡é¢˜ | {title} |
| ä½œè€… | {author} |
| å‘å¸ƒæ—¶é—´ | {publish_time} |
| å­—æ•°ç»Ÿè®¡ | {word_count}å­— |
| æ®µè½æ•°é‡ | {len(paragraphs)}æ®µ |
| ç¬”è®°æ—¥æœŸ | {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} |

## ğŸ” å…³é”®è¯åˆ†æ
{keyword_analysis}

## ğŸ“‹ å†…å®¹ç»“æ„åˆ†æ

### æ–‡ç« è„‰ç»œ
"""
    
    # æ·»åŠ æ®µè½åˆ†æ
    for i, para in enumerate(paragraphs[:10]):  # åªåˆ†æå‰10æ®µ
        if len(para) > 50:  # åªåˆ†æè¾ƒé•¿çš„æ®µè½
            notes += f"\n**ç¬¬{i+1}æ®µ**ï¼š{para[:150]}{'...' if len(para) > 150 else ''}\n"
    
    notes += f"""

## ğŸ’­ æ·±åº¦æ€è€ƒ

### ä¸»è¦è§‚ç‚¹
1. 
2. 
3. 

### è®ºè¯é€»è¾‘
- 
- 
- 

### å¯å‘ä¸æ”¶è·
- 
- 
- 

## ğŸ¯ å®è·µåº”ç”¨

### å¯è¡ŒåŠ¨çš„å»ºè®®
1. 
2. 
3. 

### éœ€è¦è¿›ä¸€æ­¥å­¦ä¹ çš„å†…å®¹
- 
- 
- 

## ğŸ“š ç›¸å…³é˜…è¯»æ¨è
- 
- 
- 

## ğŸ”– é‡è¦æ‘˜å½•
> 

---
*è¯¦ç»†ç¬”è®°æ¨¡æ¿ï¼Œè¯·æ ¹æ®ä¸ªäººç†è§£å¡«å……å…·ä½“å†…å®¹*
"""
    return notes

def _generate_mind_map_notes(title, author, publish_time, content, word_count, keywords):
    """ç”Ÿæˆæ€ç»´å¯¼å›¾å¼è¯»ä¹¦ç¬”è®°"""
    # æå–ä¸»è¦æ¦‚å¿µ
    main_concepts = [kw.get("word", "") for kw in keywords[:6]]
    
    notes = f"""# æ€ç»´å¯¼å›¾å¼ç¬”è®°ï¼š{title}

```
{title}
â”œâ”€â”€ ğŸ“š åŸºæœ¬ä¿¡æ¯
â”‚   â”œâ”€â”€ ä½œè€…ï¼š{author}
â”‚   â”œâ”€â”€ æ—¶é—´ï¼š{publish_time}
â”‚   â””â”€â”€ å­—æ•°ï¼š{word_count}å­—
â”‚
â”œâ”€â”€ ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ
â”‚   â”œâ”€â”€ {main_concepts[0] if len(main_concepts) > 0 else 'æ¦‚å¿µ1'}
â”‚   â”œâ”€â”€ {main_concepts[1] if len(main_concepts) > 1 else 'æ¦‚å¿µ2'}
â”‚   â”œâ”€â”€ {main_concepts[2] if len(main_concepts) > 2 else 'æ¦‚å¿µ3'}
â”‚   â””â”€â”€ {main_concepts[3] if len(main_concepts) > 3 else 'æ¦‚å¿µ4'}
â”‚
â”œâ”€â”€ ğŸ’¡ ä¸»è¦è§‚ç‚¹
â”‚   â”œâ”€â”€ è§‚ç‚¹1ï¼š
â”‚   â”œâ”€â”€ è§‚ç‚¹2ï¼š
â”‚   â””â”€â”€ è§‚ç‚¹3ï¼š
â”‚
â”œâ”€â”€ ğŸ”— é€»è¾‘å…³ç³»
â”‚   â”œâ”€â”€ å› æœå…³ç³»ï¼š
â”‚   â”œâ”€â”€ å¯¹æ¯”å…³ç³»ï¼š
â”‚   â””â”€â”€ é€’è¿›å…³ç³»ï¼š
â”‚
â”œâ”€â”€ ğŸ“ å®è·µè¦ç‚¹
â”‚   â”œâ”€â”€ æ–¹æ³•1ï¼š
â”‚   â”œâ”€â”€ æ–¹æ³•2ï¼š
â”‚   â””â”€â”€ æ–¹æ³•3ï¼š
â”‚
â””â”€â”€ ğŸ“ å­¦ä¹ æ”¶è·
    â”œâ”€â”€ æ–°çŸ¥è¯†ï¼š
    â”œâ”€â”€ æ–°æ–¹æ³•ï¼š
    â””â”€â”€ æ–°æ€è·¯ï¼š
```

## ğŸ§  æ€ç»´æ‹“å±•

### è”æƒ³ç½‘ç»œ
- {main_concepts[0] if len(main_concepts) > 0 else 'æ ¸å¿ƒæ¦‚å¿µ'} â†’ 
- {main_concepts[1] if len(main_concepts) > 1 else 'ç›¸å…³æ¦‚å¿µ'} â†’ 
- {main_concepts[2] if len(main_concepts) > 2 else 'å»¶ä¼¸æ¦‚å¿µ'} â†’ 

### é—®é¢˜æ€è€ƒ
1. ä¸ºä»€ä¹ˆï¼Ÿ
2. å¦‚ä½•åšï¼Ÿ
3. è¿˜æœ‰ä»€ä¹ˆï¼Ÿ

---
*æ€ç»´å¯¼å›¾å¸®åŠ©å»ºç«‹çŸ¥è¯†é—´çš„è¿æ¥ï¼Œä¿ƒè¿›æ·±åº¦ç†è§£*
"""
    return notes

def _generate_key_points_notes(title, author, publish_time, content, word_count, keywords):
    """ç”Ÿæˆè¦ç‚¹å¼è¯»ä¹¦ç¬”è®°"""
    # æå–å…³é”®å¥å­
    sentences = [s.strip() for s in content.replace('ã€‚', 'ã€‚\n').split('\n') if s.strip() and len(s.strip()) > 15]
    key_sentences = sentences[:8]  # å–å‰8ä¸ªå…³é”®å¥å­
    
    notes = f"""# è¦ç‚¹å¼ç¬”è®°ï¼š{title}

## â„¹ï¸ æ–‡ç« é€Ÿè§ˆ
**æ ‡é¢˜**ï¼š{title}  
**ä½œè€…**ï¼š{author}  
**æ—¶é—´**ï¼š{publish_time}  
**å­—æ•°**ï¼š{word_count}å­—  

## ğŸ¯ æ ¸å¿ƒè¦ç‚¹

### å…³é”®è¯é¢‘åˆ†æ
"""
    
    for i, kw in enumerate(keywords[:8], 1):
        notes += f"{i}. **{kw.get('word', '')}** (å‡ºç°{kw.get('count', 0)}æ¬¡)\n"
    
    notes += f"""
### é‡è¦è§‚ç‚¹æå–
"""
    
    for i, sentence in enumerate(key_sentences, 1):
        if len(sentence) > 100:
            notes += f"{i}. {sentence[:100]}...\n"
        else:
            notes += f"{i}. {sentence}\n"
    
    notes += f"""

## âœ… è¡ŒåŠ¨æ¸…å•

### ç«‹å³å¯åš
- [ ] 
- [ ] 
- [ ] 

### è®¡åˆ’æ‰§è¡Œ
- [ ] 
- [ ] 
- [ ] 

### é•¿æœŸç›®æ ‡
- [ ] 
- [ ] 
- [ ] 

## ğŸ” æ·±å…¥ç ”ç©¶æ–¹å‘
1. 
2. 
3. 

## ğŸ“Œ è®°å¿†è¦ç‚¹
- 
- 
- 

---
*è¦ç‚¹å¼ç¬”è®°ä¾¿äºå¿«é€Ÿå›é¡¾å’Œæ‰§è¡Œ*
"""
    return notes

@mcp.tool()
async def generate_one_sentence_summary(article_data: dict) -> str:
    """
    ç”Ÿæˆæ–‡ç« çš„ä¸€å¥è¯è¯»ä¹¦ç¬”è®°
    
    Args:
        article_data: æ–‡ç« æ•°æ®å¯¹è±¡ï¼ŒåŒ…å«æ ‡é¢˜ã€å†…å®¹ç­‰ä¿¡æ¯
    
    Returns:
        ä¸€å¥è¯æ€»ç»“çš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not article_data or not isinstance(article_data, dict):
            return json.dumps({
                "status": "error",
                "message": "article_data å¿…é¡»æ˜¯å­—å…¸æ ¼å¼çš„æ–‡ç« æ•°æ®"
            }, ensure_ascii=False, indent=2)
        
        logger.info("ç”Ÿæˆä¸€å¥è¯è¯»ä¹¦ç¬”è®°")
        
        # è·å–æ–‡ç« åŸºæœ¬ä¿¡æ¯
        title = article_data.get("title", "æœªçŸ¥æ ‡é¢˜")
        author = article_data.get("author", "æœªçŸ¥ä½œè€…")
        content = article_data.get("content", "")
        
        # åˆ†æå…³é”®è¯
        keywords_result = await analyze_article_content(article_data, "keywords")
        keywords_data = json.loads(keywords_result)
        keywords = keywords_data.get("keywords", [])
        
        # æå–æ ¸å¿ƒå…³é”®è¯ï¼ˆå‰3ä¸ªï¼‰
        core_keywords = [kw.get("word", "") for kw in keywords[:3]]
        
        # æå–æ–‡ç« çš„ç¬¬ä¸€æ®µå’Œæœ€åä¸€æ®µä½œä¸ºæ ¸å¿ƒè§‚ç‚¹
        paragraphs = [p.strip() for p in content.split('\n') if p.strip() and len(p.strip()) > 20]
        
        # ç”Ÿæˆä¸€å¥è¯æ€»ç»“
        one_sentence = _generate_one_sentence_from_content(title, author, core_keywords, paragraphs)
        
        return json.dumps({
            "status": "success",
            "title": title,
            "author": author,
            "core_keywords": core_keywords,
            "one_sentence_summary": one_sentence,
            "generated_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆä¸€å¥è¯è¯»ä¹¦ç¬”è®°å¤±è´¥: {e}")
        return json.dumps({
            "status": "error",
            "message": f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)

def _generate_one_sentence_from_content(title, author, keywords, paragraphs):
    """æ ¹æ®æ–‡ç« å†…å®¹ç”Ÿæˆä¸€å¥è¯æ€»ç»“"""
    
    # åˆ†ææ ‡é¢˜ä¸­çš„å…³é”®ä¿¡æ¯
    title_clean = title.replace('èŠä¸€èŠ', '').replace('æˆ‘æ˜¯å¦‚ä½•', '').replace('çš„', '')
    
    # æ„å»ºæ ¸å¿ƒä¸»é¢˜
    if keywords:
        main_topic = keywords[0]  # ç¬¬ä¸€ä¸ªå…³é”®è¯ä½œä¸ºä¸»é¢˜
        supporting_topics = "ã€".join(keywords[1:3]) if len(keywords) > 1 else ""
    else:
        main_topic = title_clean
        supporting_topics = ""
    
    # æ ¹æ®ä¸åŒç±»å‹çš„æ–‡ç« ç”Ÿæˆä¸åŒçš„æ€»ç»“æ¨¡å¼
    if "å­¦ä¹ " in title or "å¦‚ä½•" in title:
        # å­¦ä¹ æ–¹æ³•ç±»æ–‡ç« 
        if supporting_topics:
            summary = f"ä½œè€…{author}åˆ†äº«äº†å­¦ä¹ {main_topic}çš„æ–¹æ³•ï¼Œé‡ç‚¹å¼ºè°ƒäº†{supporting_topics}çš„é‡è¦æ€§ï¼Œå»ºè®®é€šè¿‡å®è·µå’ŒæŒç»­å­¦ä¹ æ¥æŒæ¡ç›¸å…³æŠ€èƒ½ã€‚"
        else:
            summary = f"ä½œè€…{author}åˆ†äº«äº†å­¦ä¹ {main_topic}çš„ç»éªŒå’Œæ–¹æ³•ï¼Œå¼ºè°ƒäº†å®è·µçš„é‡è¦æ€§ã€‚"
    
    elif "æŠ€æœ¯" in title or "å¼€å‘" in title:
        # æŠ€æœ¯ç±»æ–‡ç« 
        if supporting_topics:
            summary = f"è¿™ç¯‡æ–‡ç« ä»‹ç»äº†{main_topic}æŠ€æœ¯ï¼Œæ¶µç›–äº†{supporting_topics}ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºå¼€å‘è€…æä¾›äº†å®ç”¨çš„æŠ€æœ¯æŒ‡å¯¼ã€‚"
        else:
            summary = f"è¿™ç¯‡æ–‡ç« ä»‹ç»äº†{main_topic}ç›¸å…³çš„æŠ€æœ¯å†…å®¹å’Œå®è·µç»éªŒã€‚"
    
    elif "æ€è€ƒ" in title or "è§‚ç‚¹" in title or "çœ‹æ³•" in title:
        # è§‚ç‚¹æ€è€ƒç±»æ–‡ç« 
        if supporting_topics:
            summary = f"ä½œè€…{author}å¯¹{main_topic}è¿›è¡Œäº†æ·±å…¥æ€è€ƒï¼Œä»{supporting_topics}ç­‰è§’åº¦åˆ†æäº†ç›¸å…³é—®é¢˜ï¼Œæå‡ºäº†ç‹¬åˆ°çš„è§è§£ã€‚"
        else:
            summary = f"ä½œè€…{author}å¯¹{main_topic}è¿›è¡Œäº†æ·±å…¥æ€è€ƒï¼Œåˆ†äº«äº†ä¸ªäººçš„è§‚ç‚¹å’Œè§è§£ã€‚"
    
    elif "ä»‹ç»" in title or "ä»€ä¹ˆæ˜¯" in title:
        # ä»‹ç»è¯´æ˜ç±»æ–‡ç« 
        if supporting_topics:
            summary = f"è¿™ç¯‡æ–‡ç« è¯¦ç»†ä»‹ç»äº†{main_topic}çš„æ¦‚å¿µå’Œåº”ç”¨ï¼Œé‡ç‚¹é˜è¿°äº†{supporting_topics}ç­‰å…³é”®è¦ç´ ã€‚"
        else:
            summary = f"è¿™ç¯‡æ–‡ç« è¯¦ç»†ä»‹ç»äº†{main_topic}çš„ç›¸å…³å†…å®¹å’ŒåŸºæœ¬æ¦‚å¿µã€‚"
    
    else:
        # é€šç”¨æ¨¡å¼
        if supporting_topics:
            summary = f"ä½œè€…{author}å›´ç»•{main_topic}ä¸»é¢˜ï¼Œæ·±å…¥æ¢è®¨äº†{supporting_topics}ç­‰å…³é”®é—®é¢˜ï¼Œä¸ºè¯»è€…æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œå»ºè®®ã€‚"
        else:
            summary = f"ä½œè€…{author}å›´ç»•{main_topic}ä¸»é¢˜åˆ†äº«äº†ä¸ªäººçš„ç»éªŒå’Œè§è§£ã€‚"
    
    return summary

@mcp.tool()
async def crawl_and_create_reading_notes(url: str, note_style: str = "summary", download_images: bool = True, custom_filename: str = None) -> str:
    """
    ä¸€å¥è¯å®Œæˆï¼šçˆ¬å–å¾®ä¿¡æ–‡ç« å¹¶ç”Ÿæˆè¯»ä¹¦ç¬”è®°
    
    Args:
        url: å¾®ä¿¡æ–‡ç« URL
        note_style: ç¬”è®°é£æ ¼ï¼šsummary(æ‘˜è¦å¼), detailed(è¯¦ç»†å¼), mind_map(æ€ç»´å¯¼å›¾å¼), key_points(è¦ç‚¹å¼), one_sentence(ä¸€å¥è¯æ€»ç»“)
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡ï¼Œé»˜è®¤ä¸ºTrue
        custom_filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
    
    Returns:
        å®Œæ•´æ“ä½œç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«çˆ¬å–ç»“æœã€åˆ†æç»“æœå’Œç¬”è®°å†…å®¹
    """
    try:
        logger.info(f"å¼€å§‹ä¸€ç«™å¼å¤„ç†: url={url}, note_style={note_style}")
        
        # ç¬¬ä¸€æ­¥ï¼šçˆ¬å–æ–‡ç« 
        logger.info("æ­¥éª¤1: çˆ¬å–å¾®ä¿¡æ–‡ç« ...")
        spider = WeixinSpider(download_images=download_images)
        
        article_data = spider.crawl_article_by_url(url)
        if not article_data:
            return json.dumps({
                "status": "error",
                "message": "æ–‡ç« çˆ¬å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®"
            }, ensure_ascii=False, indent=2)
        
        # ä¿å­˜çˆ¬å–çš„æ–‡ç« 
        if custom_filename:
            save_success = spider.save_article_to_file(article_data, custom_filename)
        else:
            save_success = spider.save_article_to_file(article_data)
        
        logger.info(f"æ–‡ç« çˆ¬å–å®Œæˆ: æ ‡é¢˜={article_data.get('title')}, å­—æ•°={article_data.get('word_count')}")
        
        # ç¬¬äºŒæ­¥ï¼šåˆ†ææ–‡ç« å†…å®¹
        logger.info("æ­¥éª¤2: åˆ†ææ–‡ç« å†…å®¹...")
        analysis_result = await analyze_article_content(article_data, "full")
        analysis_data = json.loads(analysis_result)
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆè¯»ä¹¦ç¬”è®°
        logger.info(f"æ­¥éª¤3: ç”Ÿæˆ{note_style}é£æ ¼è¯»ä¹¦ç¬”è®°...")
        notes_result = await generate_reading_notes(article_data, note_style)
        notes_data = json.loads(notes_result)
        
        # ç¬¬å››æ­¥ï¼šä¿å­˜è¯»ä¹¦ç¬”è®°åˆ°æ–‡ä»¶
        logger.info("æ­¥éª¤4: ä¿å­˜è¯»ä¹¦ç¬”è®°...")
        if notes_data.get("status") == "success":
            notes_content = notes_data.get("notes", "")
            
            # ç¡®ä¿reading_notesç›®å½•å­˜åœ¨
            notes_dir = "reading_notes"
            os.makedirs(notes_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            title = article_data.get("title", "æœªçŸ¥æ ‡é¢˜")
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title.replace(' ', '_')
            
            if custom_filename:
                filename = f"{custom_filename}_{note_style}ç¬”è®°.md"
            else:
                filename = f"{safe_title}_{note_style}ç¬”è®°.md"
            
            file_path = os.path.join(notes_dir, filename)
            
            # ä¿å­˜ç¬”è®°æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(notes_content)
            
            file_size = os.path.getsize(file_path)
            logger.info(f"è¯»ä¹¦ç¬”è®°å·²ä¿å­˜åˆ°: {file_path}")
        
        # è¿”å›å®Œæ•´ç»“æœ
        return json.dumps({
            "status": "success",
            "message": "æ–‡ç« çˆ¬å–ã€åˆ†æå’Œç¬”è®°ç”Ÿæˆå®Œæˆ",
            "article_info": {
                "title": article_data.get("title"),
                "author": article_data.get("author"),
                "publish_time": article_data.get("publish_time"),
                "word_count": article_data.get("word_count"),
                "images_count": len(article_data.get("images", [])),
                "download_success": sum(1 for img in article_data.get("images", []) if img.get("download_success", False))
            },
            "analysis_summary": {
                "keywords_count": len(analysis_data.get("keywords", [])) if analysis_data.get("status") == "success" else 0,
                "has_summary": "summary" in analysis_data if analysis_data.get("status") == "success" else False,
                "images_analyzed": len(analysis_data.get("images", [])) if analysis_data.get("status") == "success" else 0
            },
            "notes_info": {
                "style": note_style,
                "word_count": notes_data.get("word_count", 0),
                "file_path": file_path if notes_data.get("status") == "success" else None,
                "file_size": file_size if notes_data.get("status") == "success" else 0
            },
            "files_created": {
                "article_files": spider.get_saved_files_info() if save_success else [],
                "notes_file": file_path if notes_data.get("status") == "success" else None
            },
            "processing_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"ä¸€ç«™å¼å¤„ç†å¤±è´¥: {e}")
        return json.dumps({
            "status": "error",
            "message": f"å¤„ç†å¤±è´¥: {str(e)}",
            "step": "æœªçŸ¥æ­¥éª¤"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def create_and_save_reading_notes(article_data: dict, note_style: str = "summary", save_to_file: bool = True, custom_filename: str = None) -> str:
    """
    ç”Ÿæˆè¯»ä¹¦ç¬”è®°å¹¶ä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        article_data: æ–‡ç« æ•°æ®å¯¹è±¡ï¼ŒåŒ…å«æ ‡é¢˜ã€å†…å®¹ç­‰ä¿¡æ¯
        note_style: ç¬”è®°é£æ ¼ï¼šsummary(æ‘˜è¦å¼), detailed(è¯¦ç»†å¼), mind_map(æ€ç»´å¯¼å›¾å¼), key_points(è¦ç‚¹å¼), one_sentence(ä¸€å¥è¯æ€»ç»“)
        save_to_file: æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue
        custom_filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ“ä½œç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not article_data or not isinstance(article_data, dict):
            return json.dumps({
                "status": "error",
                "message": "article_data å¿…é¡»æ˜¯å­—å…¸æ ¼å¼çš„æ–‡ç« æ•°æ®"
            }, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ›å»ºå¹¶ä¿å­˜è¯»ä¹¦ç¬”è®°: note_style={note_style}, save_to_file={save_to_file}")
        
        # ç”Ÿæˆè¯»ä¹¦ç¬”è®°
        notes_result = await generate_reading_notes(article_data, note_style)
        notes_data = json.loads(notes_result)
        
        if notes_data.get("status") != "success":
            return notes_result
        
        notes_content = notes_data.get("notes", "")
        
        result = {
            "status": "success",
            "note_style": note_style,
            "notes_generated": True,
            "notes_word_count": len(notes_content),
            "generated_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # å¦‚æœéœ€è¦ä¿å­˜åˆ°æ–‡ä»¶
        if save_to_file:
            # ç”Ÿæˆæ–‡ä»¶å
            if not custom_filename:
                title = article_data.get("title", "æœªçŸ¥æ–‡ç« ")
                # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
                safe_title = re.sub(r'[^\w\s-]', '', title)
                safe_title = re.sub(r'[-\s]+', '-', safe_title)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                custom_filename = f"è¯»ä¹¦ç¬”è®°_{safe_title}_{note_style}_{timestamp}"
            
            # ç¡®ä¿æ–‡ä»¶åä»¥.mdç»“å°¾ï¼ˆMarkdownæ ¼å¼ï¼‰
            if not custom_filename.endswith('.md'):
                custom_filename += '.md'
            
            # åˆ›å»ºnotesç›®å½•
            notes_dir = "reading_notes"
            os.makedirs(notes_dir, exist_ok=True)
            
            # å®Œæ•´æ–‡ä»¶è·¯å¾„
            file_path = os.path.join(notes_dir, custom_filename)
            
            # ä¿å­˜æ–‡ä»¶
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(notes_content)
                
                result.update({
                    "file_saved": True,
                    "file_path": file_path,
                    "file_size": len(notes_content.encode('utf-8'))
                })
                
                logger.info(f"è¯»ä¹¦ç¬”è®°å·²ä¿å­˜åˆ°: {file_path}")
                
            except Exception as e:
                result.update({
                    "file_saved": False,
                    "save_error": str(e)
                })
                logger.error(f"ä¿å­˜ç¬”è®°æ–‡ä»¶å¤±è´¥: {e}")
        else:
            result.update({
                "file_saved": False,
                "notes_content": notes_content
            })
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå¹¶ä¿å­˜è¯»ä¹¦ç¬”è®°å¤±è´¥: {e}")
        return json.dumps({
            "status": "error",
            "message": f"æ“ä½œå¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)

@mcp.tool()
async def save_article_to_file(article_data: dict, custom_filename: str = None) -> str:
    """
    å°†æ–‡ç« æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆJSONã€TXTã€HTMLæ ¼å¼ï¼‰
    
    Args:
        article_data: æ–‡ç« æ•°æ®å¯¹è±¡
        custom_filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
    
    Returns:
        ä¿å­˜ç»“æœçš„å­—ç¬¦ä¸²
    """
    try:
        if not article_data or not isinstance(article_data, dict):
            return "é”™è¯¯ï¼šarticle_data å¿…é¡»æ˜¯å­—å…¸æ ¼å¼çš„æ–‡ç« æ•°æ®"
        
        # è·å–çˆ¬è™«å®ä¾‹
        spider = get_spider_instance()
        
        # ä¿å­˜æ–‡ç« 
        success = spider.save_article_to_file(article_data, custom_filename)
        
        if success:
            return "æ–‡ç« ä¿å­˜æˆåŠŸï¼å·²ç”Ÿæˆ JSONã€TXTã€HTML æ ¼å¼çš„æ–‡ä»¶"
        else:
            return "é”™è¯¯ï¼šæ–‡ç« ä¿å­˜å¤±è´¥"
            
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
        return f"é”™è¯¯ï¼šä¿å­˜å¤±è´¥ - {str(e)}"

def cleanup():
    """æ¸…ç†èµ„æº"""
    global spider_instance
    if spider_instance:
        try:
            spider_instance.close()
            logger.info("çˆ¬è™«å®ä¾‹å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­çˆ¬è™«å®ä¾‹å¤±è´¥: {e}")

if __name__ == "__main__":
    try:
        # æ³¨å†Œæ¸…ç†å‡½æ•°
        import atexit
        atexit.register(cleanup)
        
        logger.info("å¯åŠ¨å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬å– MCP æœåŠ¡å™¨...")
        
        # ä»¥æ ‡å‡† I/O æ–¹å¼è¿è¡Œ MCP æœåŠ¡å™¨
        mcp.run(transport='stdio')
        
    except Exception as e:
        import traceback
        logger.error(f"æœåŠ¡å™¨è¿è¡Œå¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        sys.exit(1) 